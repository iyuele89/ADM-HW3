{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Which book would you recommend?\n",
    "\n",
    "*Stefano D'Arrigo 1960500, Alessio Sentinelli, Iyuele Alemu Korsaye*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![goodreads image](./images/goodrreads.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes before starting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to keep this notebook tidy and agile to read, the majority of the code we wrote to complete the tasks is not included here and is provided into the folder `scripts`. Nevertheless, the crucial pieces of code are directly executed or shown and commented inside this notebook. For further understanding of each operation and choice we made, please refer to the comments to the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "source": [
    "#### Loading libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import re\n",
    "from selenium import webdriver\n",
    "import chromedriver_binary\n",
    "import spacy\n",
    "from spacy_fastlang import LanguageDetector"
   ]
  },
  {
   "source": [
    "### 1.1. Get the list of books"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we open a txt file and extract the URL of the 30k books, using a for loop over the 300 pages and leveraging the request library we get access to the pages where the books are present.\n",
    "\n",
    "Then using a third-party library, beautiful soup we can pull the data out of the HTML files and use the ‘lxml’ parser to extract the class \"js tooltipTrigger tooltipTrigger\" within a div tag, where the url of the books is present.\n",
    "Finally we write within a loop function the collected 30k url on the text file we opened previously and close the file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"url_list.txt\",\"w\")\n",
    "for k in range (1,301):  #301 for the pages\n",
    "    page = requests.get(\"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\" + str(k))\n",
    "    soup = BeautifulSoup(page.content, features=\"lxml\")\n",
    "    URL_con3 = soup.find_all('div', class_=\"js-tooltipTrigger tooltipTrigger\")\n",
    "    for j in range (0,100): #100 for the books\n",
    "        URL_str = str(URL_con3[j]) \n",
    "        list_split = URL_str.split(\" \")\n",
    "        result = list_split[5] # it seems it is always 5\n",
    "        result_clean = result.split(\"\\\"\")[1]\n",
    "        f.write(\"https://www.goodreads.com\" + result_clean + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained a txt file with 30k lines with each line containing the URL book, and the txt file is finally saved in the data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this task is to retrieve all the `HTML` pages of the books, reading the `url_list.txt` file that we created in the previous task.\n",
    "\n",
    "In order to bypass eventual security measures against scraping, we leveraged the library `selenium`, which provides a full and automatized web client agent. \n",
    "\n",
    "To complete this task, we wrote a class `DataCollector`, included in `data_collection.py`. The methods of this class receive the user's input, compute the offset from which start reading the URLs file and save the HTML pages. \n",
    "\n",
    "The core business of this class is included into the following method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "def __save_html_pages(self, start_from, stop_at):\n",
    "        \"\"\"\n",
    "        Start collecting from line start_from and stop at line stop_at.\n",
    "        \"\"\"\n",
    "        with open(os.path.join(self.root_dir, 'url_list.txt'), 'r') as urls_file:\n",
    "            try:\n",
    "                urls = urls_file.readlines()[start_from : ] # select the line from which start collecting\n",
    "            except:\n",
    "                print('Error: reached file end!')\n",
    "                exit(-1)\n",
    "            for url, i in zip(urls, tqdm(range(start_from, stop_at))): # \n",
    "                if i % 100 == 0:\n",
    "                    self.__make_dir(i // 100 + 1)\n",
    "                try:\n",
    "                    driver.get(url)\n",
    "                    page_html = driver.page_source\n",
    "                    with open(os.path.join(self.html_dir, f'article_{i + 1:05d}.html'), 'w') as out_file:\n",
    "                        out_file.write(page_html)\n",
    "                except:\n",
    "                    with open('./log/log.csv', 'a') as log:\n",
    "                        log.write(f'[{datetime.datetime.now()}], {i+1}, {url}\\n')\n",
    "                    continue\n",
    "            driver.close()\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the parameter `start_from`, the user can decide from which document start crawling. The eventual errors in retrieving the pages were annotated in a log file and handled manually after the execution of the script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this method are the collected data, structured in the following way:\n",
    "\n",
    "```\n",
    "- html/\n",
    "    - 1/\n",
    "        - article_00001.html\n",
    "        - article_00002.html\n",
    "        - ...\n",
    "        - article_00100.html\n",
    "    - 2/\n",
    "        - article_00101.html\n",
    "        - ...\n",
    "        - article_00200.html\n",
    "    - ...\n",
    "    - 300/\n",
    "        - article_29901.html\n",
    "        - ...\n",
    "        - article_30000.html\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have accessed the HTML content of all the 30 000 books, we are left with the task of parsing the data. Since most of the HTML data is nested, we cannot extract data simply through string processing. One needs a parser which can create a nested/tree structure of the HTML data. There are many HTML parser libraries available, the one we have used in our function (book_scraping) is ‘lxml’ parser.\n",
    "\n",
    "Now, we need to navigate and search the parse tree that we created and for this task, we will be using another third-party python library, Beautiful Soup. It is a Python library for pulling data out of HTML and XML files. A really nice feature  about the BeautifulSoup library is that it is built on the top of the HTML parsing libraries like, lxml, html5lib parser, etc. so  that BeautifulSoup object and the parser library can be created at the same time.\n",
    "soup = BeautifulSoup(html_source, features='lxml')\n",
    "\n",
    "Now, we are ready to extract all the relevant data from the HTML content that are crucial for building a book recommendation engine. The soup object contains all the data in the nested structure which can be programmatically extracted using the ‘book scraping’ function, that we have created to retrieve and save for each 30k book all the relevant information (book title, book series, book author, rating value, rating count, review count, plot, number of pages, published date, characters, settings and URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_scraping(html_source, nlp): # this takes the html content and returns a list with the useful info\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    nlp.add_pipe(LanguageDetector())\n",
    "\n",
    "    soup = BeautifulSoup(html_source, features='lxml') # instantiate a BeautifulSoup object for HTML parsing\n",
    "\n",
    "    bookTitle = soup.find_all('h1', id='bookTitle')[0].contents[0].strip() # get the book title\n",
    "\n",
    "    # if bookSeries is not present, then set it to the empty string\n",
    "    try:\n",
    "        bookSeries = soup.find_all('h2', id='bookSeries')[0].contents[1].contents[0].strip()[1:-1]\n",
    "    except:\n",
    "        bookSeries = ''\n",
    "\n",
    "    # if bookAuthors is not present, then set it to the empty string\n",
    "    try:\n",
    "        bookAuthors = soup.find_all('span', itemprop='name')[0].contents[0].strip()\n",
    "    except:\n",
    "        bookAuthors = ''\n",
    "    \n",
    "    # the plot of the book is essential; if something goes wrong with the plot, raise an error\n",
    "    try:\n",
    "        Plot = soup.find_all('div', id='description')[0].contents  # get the main tag where the plot is found \n",
    "        filter_plot = list(filter(lambda i: i!='\\n', Plot))  # filter the plot by removing tags that doesn’t contain the description \n",
    "        if len(filter_plot) == 1:    \n",
    "            Plot = filter_plot[0].text\n",
    "        else:                                    # getting all the plot within the tag\n",
    "            Plot = filter_plot[1].text                                               \n",
    "    except:\n",
    "        Plot = ''                                # return an empty string if there is no description\n",
    "\n",
    "    doc = nlp(Plot)\n",
    "    if doc._.language == 'en':          # return an empty string if the description is not written in English\n",
    "        pass\n",
    "    else:\n",
    "        Plot = ''\n",
    "\n",
    "    # if NumberofPages is not present, then set it to the empty string\n",
    "    try:\n",
    "        NumberofPages = soup.find_all('span', itemprop='numberOfPages')[0].contents[0].split()[0]\n",
    "    except:\n",
    "        NumberofPages = ''\n",
    "    \n",
    "    # if ratingValue is not present, then set it to the empty string\n",
    "    try:\n",
    "        ratingValue = soup.find_all('span', itemprop='ratingValue')[0].contents[0].strip()\n",
    "    except:\n",
    "        ratingValue = ''\n",
    "    \n",
    "    # if rating_reviews is not present, then set it to the empty string\n",
    "    try:\n",
    "        ratings_reviews = soup.find_all('a', href='#other_reviews')\n",
    "        for i in ratings_reviews:\n",
    "            if i.find_all('meta',itemprop='ratingCount'):\n",
    "                ratingCount = i.contents[2].split()[0]\n",
    "            if i.find_all('meta',itemprop='reviewCount'):\n",
    "                reviewCount = i.contents[2].split()[0]\n",
    "    except:\n",
    "        ratings_reviews = ''\n",
    "\n",
    "    # if Published is not present, then set it to the empty string\n",
    "    try:        \n",
    "        pub = soup.find_all('div', class_='row')[1].contents[0].split()[1:4]\n",
    "        Published = ' '.join(pub) # join the list of publishers\n",
    "    except:\n",
    "        Published = ''\n",
    "    \n",
    "    # if Character is not present, then set it to the empty string\n",
    "    try:\n",
    "        char = soup.find_all('a', href=re.compile('characters')) # find the regular expression(re) 'characters' within the attribute href \n",
    "        if len(char) == 0:\n",
    "            Characters = '' # no characters in char\n",
    "        else:\n",
    "            Characters = ', '.join([i.contents[0] for i in char])\n",
    "    except:\n",
    "        Characters = '' # something went wrong with char\n",
    "    \n",
    "    # if Setting is not present, then set it to the empty string\n",
    "    try:\n",
    "        sett = soup.find_all('a', href=re.compile('places')) # find the regular expression(re) 'places' within the attribute href \n",
    "        if len(sett) == 0:\n",
    "            Setting = ''\n",
    "        else:\n",
    "            Setting = ', '.join([i.contents[0] for i in sett])\n",
    "    except:\n",
    "        Setting = '' # something went wrong with Setting\n",
    "    \n",
    "    # get the URL to the page\n",
    "    Url = soup.find_all('link', rel='canonical')[0].get('href')\n",
    "\n",
    "    return [bookTitle, bookSeries, bookAuthors, ratingValue, ratingCount, reviewCount, Plot, NumberofPages, Published, Characters, Setting, Url]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the function is structured in a manner that for each book, the extracted relevant information are in a tab separated values extensions ready to be feed-in for the next stage.\n",
    "During the scraping procedure, if the information we were seeking was not available then an empty string is returned and also books with a description written in a different language than in English were discarded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithmic Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "s1 = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "s2 = 'CADFECEILGJHABNOPSTIRYOEABILCNR'\n",
    "m = len(s1) + 1\n",
    "n = len(s2) + 1\n",
    "e_matrix = np.zeros((m, n))\n",
    "#for i in range(0, m):\n",
    "#    e_matrix[i][0] = i\n",
    "#for j in range(0, n):\n",
    "#    e_matrix[0][j] = j\n",
    "for i in range(1, m):\n",
    "    for j in range(1, n):\n",
    "        if s1[i-1] == s2[j-1]:\n",
    "            e_matrix[i][j] = e_matrix[i-1][j-1] + 1\n",
    "        elif e_matrix[i-1][j] > e_matrix[i][j-1]:\n",
    "            e_matrix[i][j] = e_matrix[i-1][j]\n",
    "        else:\n",
    "            e_matrix[i][j] = e_matrix[i][j-1]\n",
    "print(e_matrix[m-1][n-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(s, i):\n",
    "    l = [0]\n",
    "    for j in range(0, i):\n",
    "        if s[j] < s[i]:\n",
    "            l.append(max_length(s, j))\n",
    "    return max(l) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'ZQABWARSTA' # \n",
    "max_length(s, len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = []\n",
    "for i in range(0, len(s)):\n",
    "    l.append(max_length(s, i))\n",
    "max(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
