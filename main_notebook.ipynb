{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - Which book would you recommend?\n",
    "\n",
    "*Stefano D'Arrigo 1960500, Alessio Sentinelli, Iyuele*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![goodreads image](./images/goodrreads.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes before starting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to keep this notebook tidy and agile to read, the majority of the code we wrote to complete the tasks is not included here and is provided into the folder `scripts`. Nevertheless, the crucial pieces of code are directly executed or shown and commented inside this notebook. For further understanding of each operation and choice we made, please refer to the comments to the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Get the list of books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please describe here what are the characteristics of the website structure you exploited to get the needed information, what are the choices you made and what you did. Comment and refer to the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "\n",
    "f = open(\"url_list.txt\",\"w\")\n",
    "for k in range (1,301):  #301 for the pages\n",
    "    page = requests.get(\"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\" + str(k))\n",
    "    soup = BeautifulSoup(page.content, features=\"lxml\")\n",
    "    URL_con3 = soup.find_all('div', class_=\"js-tooltipTrigger tooltipTrigger\")\n",
    "    for j in range (0,100): #100 for the books\n",
    "        URL_str = str(URL_con3[j]) \n",
    "        list_split = URL_str.split(\" \")\n",
    "        result = list_split[5] # it seems it is always 5\n",
    "        result_clean = result.split(\"\\\"\")[1]\n",
    "        f.write(\"https://www.goodreads.com\" + result_clean + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment briefly on the output of this task and say where the reader can find the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Crawl books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this task is to retrieve all the `HTML` pages of the books, reading the `url_list.txt` file that we created in the previous task.\n",
    "\n",
    "In order to bypass eventual security measures against scraping, we leveraged the library `selenium`, which provides a full and automatized web client agent. \n",
    "\n",
    "To complete this task, we wrote a class `DataCollector`, included in `data_collection.py`. The methods of this class receive the user's input, compute the offset from which start reading the URLs file and save the HTML pages. \n",
    "\n",
    "The core business of this class is included into the following method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~python\n",
    "def __save_html_pages(self, start_from, stop_at):\n",
    "        \"\"\"\n",
    "        Start collecting from line start_from and stop at line stop_at.\n",
    "        \"\"\"\n",
    "        with open(os.path.join(self.root_dir, 'url_list.txt'), 'r') as urls_file:\n",
    "            try:\n",
    "                urls = urls_file.readlines()[start_from : ] # select the line from which start collecting\n",
    "            except:\n",
    "                print('Error: reached file end!')\n",
    "                exit(-1)\n",
    "            for url, i in zip(urls, tqdm(range(start_from, stop_at))): # \n",
    "                if i % 100 == 0:\n",
    "                    self.__make_dir(i // 100 + 1)\n",
    "                try:\n",
    "                    driver.get(url)\n",
    "                    page_html = driver.page_source\n",
    "                    with open(os.path.join(self.html_dir, f'article_{i + 1:05d}.html'), 'w') as out_file:\n",
    "                        out_file.write(page_html)\n",
    "                except:\n",
    "                    with open('./log/log.csv', 'a') as log:\n",
    "                        log.write(f'[{datetime.datetime.now()}], {i+1}, {url}\\n')\n",
    "                    continue\n",
    "            driver.close()\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the parameter `start_from`, the user can decide from which document start crawling. The eventual errors in retrieving the pages were annotated in a log file and handled manually after the execution of the script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this method are the collected data, structured in the following way:\n",
    "\n",
    "```\n",
    "- html/\n",
    "    - 1/\n",
    "        - article_00001.html\n",
    "        - article_00002.html\n",
    "        - ...\n",
    "        - article_00100.html\n",
    "    - 2/\n",
    "        - article_00101.html\n",
    "        - ...\n",
    "        - article_00200.html\n",
    "    - ...\n",
    "    - 300/\n",
    "        - article_29901.html\n",
    "        - ...\n",
    "        - article_30000.html\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please describe here what are the characteristics of the website structure you exploited to get the needed information, what are the choices you made and what you did. Comment and refer to the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def book_scraping(html_source): # this takes the html content and returns a list with the useful info\n",
    "\n",
    "    soup = BeautifulSoup(html_source, features='lxml') # instantiate a BeautifulSoup object for HTML parsing\n",
    "\n",
    "    bookTitle = soup.find_all('h1', id='bookTitle')[0].contents[0].strip() # get the book title\n",
    "\n",
    "    # if bookSeries is not present, then set it to the empty string\n",
    "    try:\n",
    "        bookSeries = soup.find_all('h2', id='bookSeries')[0].contents[1].contents[0].strip()[1:-1]\n",
    "    except:\n",
    "        bookSeries = ''\n",
    "\n",
    "    # if bookAuthors is not present, then set it to the empty string\n",
    "    try:\n",
    "        bookAuthors = soup.find_all('span', itemprop='name')[0].contents[0].strip()\n",
    "    except:\n",
    "        bookAuthors = ''\n",
    "    \n",
    "    # the plot of the book is essential; if something goes wrong with the plot, raise an error\n",
    "    try:\n",
    "        descr = soup.find_all('div', id='description')[0].contents # get the plot\n",
    "        descr_fil= list(filter(lambda s: s!='\\n', descr)) # clean it from newline chars\n",
    "        if len(descr_fil) == 1:\n",
    "            Plot = ''.join(descr_fil[0].contents[0]) # join the filtered plot into a string\n",
    "        else:\n",
    "            descr_fil = descr_fil[1:-1]\n",
    "            x = [j for i in descr_fil for j in i.contents if (isinstance(j, str)==True)]\n",
    "            Plot = ''.join(x) # join the filtered plot into a string\n",
    "        if detect(Plot) != 'en':\n",
    "            raise Exception # if the plot is not in english, raise an error\n",
    "    except:\n",
    "        raise # pass the error to the caller function\n",
    "\n",
    "    # if NumberofPages is not present, then set it to the empty string\n",
    "    try:\n",
    "        NumberofPages = soup.find_all('span', itemprop='numberOfPages')[0].contents[0].split()[0]\n",
    "    except:\n",
    "        NumberofPages = ''\n",
    "    \n",
    "    # if ratingValue is not present, then set it to the empty string\n",
    "    try:\n",
    "        ratingValue = soup.find_all('span', itemprop='ratingValue')[0].contents[0].strip()\n",
    "    except:\n",
    "        ratingValue = ''\n",
    "    \n",
    "    # if rating_reviews is not present, then set it to the empty string\n",
    "    try:\n",
    "        ratings_reviews = soup.find_all('a', href='#other_reviews')\n",
    "        for i in ratings_reviews:\n",
    "            if i.find_all('meta',itemprop='ratingCount'):\n",
    "                ratingCount = i.contents[2].split()[0]\n",
    "            if i.find_all('meta',itemprop='reviewCount'):\n",
    "                reviewCount = i.contents[2].split()[0]\n",
    "    except:\n",
    "        ratings_reviews = ''\n",
    "\n",
    "    # if Published is not present, then set it to the empty string\n",
    "    try:        \n",
    "        pub = soup.find_all('div', class_='row')[1].contents[0].split()[1:4]\n",
    "        Published = ' '.join(pub) # join the list of publishers\n",
    "    except:\n",
    "        Published = ''\n",
    "    \n",
    "    # if Character is not present, then set it to the empty string\n",
    "    try:\n",
    "        char = soup.find_all('a', href=re.compile('characters')) # find the regular expression(re) 'characters' within the attribute href \n",
    "        if len(char) == 0:\n",
    "            Characters = '' # no characters in char\n",
    "        else:\n",
    "            Characters = ', '.join([i.contents[0] for i in char])\n",
    "    except:\n",
    "        Characters = '' # something went wrong with char\n",
    "    \n",
    "    # if Setting is not present, then set it to the empty string\n",
    "    try:\n",
    "        sett = soup.find_all('a', href=re.compile('places')) # find the regular expression(re) 'places' within the attribute href \n",
    "        if len(sett) == 0:\n",
    "            Setting = ''\n",
    "        else:\n",
    "            Setting = ', '.join([i.contents[0] for i in sett])\n",
    "    except:\n",
    "        Setting = '' # something went wrong with Setting\n",
    "    \n",
    "    # get the URL to the page\n",
    "    Url = soup.find_all('link', rel='canonical')[0].get('href')\n",
    "\n",
    "    return [bookTitle, bookSeries, bookAuthors, ratingValue, ratingCount, reviewCount, Plot, NumberofPages, Published, Characters, Setting, Url]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment briefly on the output of this task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
